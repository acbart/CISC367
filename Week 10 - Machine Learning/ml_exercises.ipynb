{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serial-alexandria",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Fill in your name and the link to this file on your github.\n",
    "\n",
    "* Name: Megan Englert\n",
    "* Link to github URL:https://github.com/meganenglert/CISC367/tree/main/Week%2010%20-%20Machine%20Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chinese-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-cabin",
   "metadata": {},
   "source": [
    "# ML: Linear Regression\n",
    "\n",
    "So this starts with linear regression. If you want a deeper dive than what I cover in class, you can refer to [this page](https://realpython.com/linear-regression-in-python/)\n",
    "\n",
    "The exercises come from this workbook, which has somewhat helpful explanations too: https://csmastersuh.github.io/data_analysis_with_python_2020/linear_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-oklahoma",
   "metadata": {},
   "source": [
    "# Exercise 10: Linear Regression\n",
    "\n",
    "You'll need to make up some data for this. Don't spend too much time on this one, it's less interesting compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "authentic-royal",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.52631579,  1.05263158,  1.57894737,  2.10526316,\n",
       "        2.63157895,  3.15789474,  3.68421053,  4.21052632,  4.73684211,\n",
       "        5.26315789,  5.78947368,  6.31578947,  6.84210526,  7.36842105,\n",
       "        7.89473684,  8.42105263,  8.94736842,  9.47368421, 10.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.35928154,  4.49670487,  2.24529572,  3.54998061,  6.58479447,\n",
       "        6.49826708,  6.63475132, 10.18517733,  9.6655279 , 11.28002895,\n",
       "       11.82023135, 11.87245996, 14.44746973, 14.20402941, 14.46479483,\n",
       "       15.67493295, 18.29765986, 19.44141067, 21.10024637, 20.95475371])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: 1.9662620163747186\n",
      "Intercept: 1.3575798486366644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "n=20\n",
    "# Linearly increasing x values\n",
    "x = np.linspace(0, 10, n)\n",
    "# Wonky line of points\n",
    "y = x*2 + 1 + 1*np.random.randn(n)\n",
    "display(x, y)\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "\n",
    "def fit_line(x,y) -> (int):    \n",
    "    model=LinearRegression(fit_intercept=True)\n",
    "    model.fit(x[:,np.newaxis], y)\n",
    "    return (model.coef_[0],model.intercept_)\n",
    "\n",
    "def main():\n",
    "    print(\"Slope:\",fit_line(x,y)[0])\n",
    "    print(\"Intercept:\", fit_line(x,y)[1])\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-dodge",
   "metadata": {},
   "source": [
    "# Exercise 11: Mystery Data\n",
    "\n",
    "This one is far more interesting. You can download the file from [here](https://raw.githubusercontent.com/AnkS4/hy-data-analysis-with-python-2020/master/part05-e11_mystery_data/src/mystery_data.tsv). Make sure it gets the right filename!\n",
    "\n",
    "You don't need to define any functions, as they demand, although you might find that helpful to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "searching-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of X1: 3.00\n",
      "Coefficient of X2: -1.00\n",
      "Coefficient of X3: 7.00\n",
      "Coefficient of X4: -0.00\n",
      "Coefficient of X5: -20.00\n"
     ]
    }
   ],
   "source": [
    "array = np.loadtxt(fname = \"mystery_data.tsv\", delimiter = '\\t', skiprows = 1)\n",
    "\n",
    "model2=LinearRegression(fit_intercept=False)\n",
    "x = array[:, :-1]\n",
    "y = array[:,-1]\n",
    "model2.fit(x,y)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Coefficient of X%d: %.2f\" % (i +1, model2.coef_[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-internet",
   "metadata": {},
   "source": [
    "## Exercise 12: Coefficient of Determination\n",
    "\n",
    "Read over this entire problem, parts 1 and 2.\n",
    "\n",
    "This reuses the same `mystery_data.tsv` file as before.\n",
    "\n",
    "Again, you do not need to define their function. Just calculate the R2 scores and print them, as they direct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "computational-marathon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score with features X: 1.0\n",
      "R2 Score with feature X1: 0.02\n",
      "R2 Score with feature X2: 0.01\n",
      "R2 Score with feature X3: 0.09\n",
      "R2 Score with feature X4: 0.00\n",
      "R2 Score with feature X5: 0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LinearRegression(fit_intercept=True)\n",
    "model.fit(x,y)\n",
    "print(\"R2 Score with features X: \" + str(model.score(x,y)))\n",
    "for i in range(5):\n",
    "    model.fit(x[:,i].reshape(-1,1),y)\n",
    "    r2 = model.score(x[:,i].reshape(-1,1),y)\n",
    "    print(\"R2 Score with feature X%d: %.2f\" % (i+1, r2))\n",
    "    \n",
    "\"\"\"\n",
    "Based on the output, I think really only X2 and X5 are needed for the model. Since they provide a good amount \n",
    "of the variability on their own, we could have a much simpler function and still a pretty solid fit with only \n",
    "those two.\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-property",
   "metadata": {},
   "source": [
    "## Exercise 13: Cycling Weather\n",
    "\n",
    "I've already prepared the data that they require for this assignment. You can download it [here](https://gist.githubusercontent.com/acbart/466174a04e9a2505c4c25f91fc6dd4f6/raw/726865070677ec7dede17a08095624e0ea35e7cd/biking.csv).\n",
    "\n",
    "The first column is the index, you can safely ignore it. The next 7 columns are straightforward. The last few columns are locations in Finland that have measuring stations. I recommend using `Baana` as they say in the instructions for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "paperback-contributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring station:  Baana\n",
      "Regression coefficient for 'precipiation': -52.179548960018785\n",
      "Regression coefficient for 'snow depth': -32.93766860685709\n",
      "Regression coefficient for 'temperature': 169.24225037357726\n",
      "Score: 0.5750246253469835\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"biking.csv\")\n",
    "def cycling_weather_continues(place: str)->((int), int):\n",
    "    x = df.loc[:, ['Precipitation amount (mm)','Snow depth (cm)','Air temperature (degC)']]\n",
    "    y = df.loc[:, place]\n",
    "    model.fit(x,y)\n",
    "    return model.coef_, model.score(x,y)\n",
    "\n",
    "def main():\n",
    "    coeffs, score = cycling_weather_continues(\"Baana\")\n",
    "    print(\"Measuring station: \", \"Baana\")\n",
    "    print(\"Regression coefficient for 'precipiation': \" + str(coeffs[0]))\n",
    "    print(\"Regression coefficient for 'snow depth': \" + str(coeffs[1]))\n",
    "    print(\"Regression coefficient for 'temperature': \" + str(coeffs[2]))\n",
    "    print(\"Score: \" + str(score))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-drunk",
   "metadata": {},
   "source": [
    "# ML Naive Bayes Classification\n",
    "\n",
    "This is the next section of the exercises, from: https://csmastersuh.github.io/data_analysis_with_python_2020/bayes.html\n",
    "\n",
    "In addition to the reading, I recommend this video: https://www.youtube.com/watch?v=CPqOCI0ahss\n",
    "\n",
    "\n",
    "## Exercise 1: Blob Classification\n",
    "\n",
    "(**OPTIONAL**) This one is very vague, and they're actually asking you to generate your own test data using the `make_blobs` function from `sklearn`'s `datasets` submodule. I've already started that work for you. But honestly if you want to skip it, I don't think it's a helpful starting question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "convinced-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is None\n",
      "array([[2.0, 2.0, 0.0, 2.5, None],\n",
      "       [2.0, 3.0, 1.0, 1.5, None],\n",
      "       [2.0, 2.0, 6.0, 3.5, None],\n",
      "       [2.0, 2.0, 3.0, 1.2, None],\n",
      "       [2.0, 4.0, 4.0, 2.7, None]], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "def blob_classification(X, y):\n",
    "    # Put ML stuff here\n",
    "    pass\n",
    "\n",
    "# Create the training data and validation data\n",
    "X, y = datasets.make_blobs(100, 2, centers=2, random_state=2, cluster_std=2.5)\n",
    "# Run your ML predictions\n",
    "print(\"The accuracy score is\", blob_classification(X, y))\n",
    "# Run this on some new data\n",
    "a=np.array([[2, 2, 0, 2.5],\n",
    "            [2, 3, 1, 1.5],\n",
    "            [2, 2, 6, 3.5],\n",
    "            [2, 2, 3, 1.2],\n",
    "            [2, 4, 4, 2.7]])\n",
    "accuracies = []\n",
    "for row in a:\n",
    "    X,y = datasets.make_blobs(100, int(row[0]), centers=int(row[1]),\n",
    "                              random_state=int(row[2]), cluster_std=row[3])\n",
    "    accuracies.append(blob_classification(X, y))\n",
    "print(repr(np.hstack([a, np.array(accuracies)[:,np.newaxis]])))\n",
    "# The last column should be the categorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-address",
   "metadata": {},
   "source": [
    "## Exercise 2: Plant Classification\n",
    "\n",
    "This is a much better question. The Iris dataset is a classic: https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "The wikipedia page gives an example of how to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adverse-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "def plant_classification():\n",
    "    df = datasets.load_iris()\n",
    "    data = df['data']\n",
    "    target = df['target']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=0)\n",
    "    model = GaussianNB()\n",
    "    model.fit(x_train,y_train)\n",
    "    y_fitted = model.predict(x_test)\n",
    "    return metrics.accuracy_score(y_test,y_fitted)\n",
    "\n",
    "print(\"Accuracy: \" + str(plant_classification()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-jersey",
   "metadata": {},
   "source": [
    "## Exercise 3: Word Classification\n",
    "\n",
    "(**Skip**)\n",
    "\n",
    "This one is too much. They give some of the data as an XML file. It's an interesting problem, and you can find the data (and solution) [here](https://github.com/AnkS4/hy-data-analysis-with-python-2020/tree/master/part06-e03_word_classification/src) if you want to tackle it, but I'm skipping it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-parcel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-invite",
   "metadata": {},
   "source": [
    "## Exercise 4: Spam Detection\n",
    "\n",
    "Download [ham.txt.gz](https://github.com/AnkS4/hy-data-analysis-with-python-2020/raw/master/part06-e04_spam_detection/src/ham.txt.gz) and [spam.txt.gz](https://github.com/AnkS4/hy-data-analysis-with-python-2020/raw/master/part06-e04_spam_detection/src/spam.txt.gz).\n",
    "\n",
    "This one is much more interesting and reasonable. It requires processing some large text files, but that's actually the easiest part, as shown in the code below. The idea is that you have spam (bad emails) and ham (good emails), and you want to determine which is which. I've done similar email processing (detecting job ads for a conference) and I was impressed with how easily I could train a little data and get very good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conceptual-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails loaded as strings: 500\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "# Load the spam emails as strings in a list.\n",
    "with gzip.open('spam.txt.gz', 'rb') as spam_file:\n",
    "    spam = spam_file.readlines()\n",
    "print(\"Number of spam emails loaded as strings:\", len(spam))\n",
    "\n",
    "# Now do the same thing with the `ham.txt.gz`\n",
    "\n",
    "# And then do the actual ML stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-heating",
   "metadata": {},
   "source": [
    "# ML Clustering\n",
    "\n",
    "This is the last section: https://csmastersuh.github.io/data_analysis_with_python_2020/clustering.html\n",
    "\n",
    "This section is one of the most interesting in my opinion. K-Means is a pretty straightforward tool, and is really worth learning how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-helmet",
   "metadata": {},
   "source": [
    "## Exercise 5: Plant Clustering\n",
    "\n",
    "Same deal as before; use the IRIS dataset. Since this has so many parameters, it can be tricky to make a good visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-raise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "urban-conclusion",
   "metadata": {},
   "source": [
    "## Exercise 6: Non-convex Clusters\n",
    "\n",
    "The data for this question is [here](https://raw.githubusercontent.com/AnkS4/hy-data-analysis-with-python-2020/master/part06-e06_nonconvex_clusters/src/data.tsv).\n",
    "\n",
    "This one shows off a different clustering algorithm ([`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)), which is \"Good for data which contains clusters of similar density\". I wasn't very familiar with DBSCAN, but it does seem much better than KMeans. It doesn't require you to figure out the number of clusters, and seems to be tricked less by unusual data. [This page](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html) was very helpful in breaking that difference down.\n",
    "\n",
    "The reference answer uses a `for` loop and `np.arange` to try `e` values from 0.05 to 0.2 in 0.05 increments, but I don't mind if you just manually try some different `e` values.\n",
    "\n",
    "Please do make a visualization with clusters colored, since I think that really highlights what we are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-maldives",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bored-circular",
   "metadata": {},
   "source": [
    "## Exercise 7: Binding Sites\n",
    "\n",
    "Download the [`data.seq` file](https://raw.githubusercontent.com/AnkS4/hy-data-analysis-with-python-2020/master/part06-e07_binding_sites/src/data.seq); note that it is just a plain textual data file, despite the fancy extension.\n",
    "\n",
    "They ask you to define `get_features_and_labels` to accept a filename, even though there's only one test file. Up to you if you want to hardcode the file path in or make it a flexible function.\n",
    "\n",
    "There are multiple parts here, and they ask you to compare the euclidean and hamming distance. I think it's worth thinking about - if you don't get what they mean, do ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "freelance-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The `find_permutation` function provided in the text, for your convenience\n",
    "def find_permutation(n_clusters, real_labels, labels):\n",
    "    permutation=[]\n",
    "    for i in range(n_clusters):\n",
    "        idx = labels == i\n",
    "        # Choose the most common label among data points in the cluster\n",
    "        new_label=scipy.stats.mode(real_labels[idx])[0][0]\n",
    "        permutation.append(new_label)\n",
    "    return permutation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
